{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9NlzZBG5zQJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the CSV file\n",
        "!wget -O dataset.csv https://raw.githubusercontent.com/okasugiarta/SATRIA-DATA/main/data/dataset_penyisihan_bdc_2024.csv\n",
        "\n",
        "\n",
        "# Open the file and read the first few lines to inspect the file\n",
        "with open('dataset.csv', 'r') as csvfile:\n",
        "    csvreader = csv.reader(csvfile)\n",
        "    print(\"Inspecting the first few lines of the file:\")\n",
        "    for i in range(5):\n",
        "        print(next(csvreader))\n",
        "\n",
        "\n",
        "try:\n",
        "    # Try reading the file with default delimiter\n",
        "    df = pd.read_csv('dataset.csv')\n",
        "except pd.errors.ParserError:\n",
        "    # If there's a parsing error, try a different delimiter or read without header\n",
        "    df = pd.read_csv('dataset.csv', delimiter=';')  # Adjust delimiter if needed\n",
        "    print(\"\\nFile read with an alternative delimiter.\\n\")\n",
        "\n",
        "print(\"\\nFirst line (header) looks like this:\")\n",
        "print(df.columns)\n",
        "\n",
        "print(\"\\nEach data point looks like this:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WVXBG_563PA",
        "outputId": "34c1175a-a375-4681-c018-1b7d326cb418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-16 03:34:02--  https://raw.githubusercontent.com/okasugiarta/SATRIA-DATA/main/data/dataset_penyisihan_bdc_2024.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1435985 (1.4M) [text/plain]\n",
            "Saving to: ‘dataset.csv’\n",
            "\n",
            "\rdataset.csv           0%[                    ]       0  --.-KB/s               \rdataset.csv         100%[===================>]   1.37M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-06-16 03:34:02 (20.9 MB/s) - ‘dataset.csv’ saved [1435985/1435985]\n",
            "\n",
            "Inspecting the first few lines of the file:\n",
            "['text;label']\n",
            "['Kunjungan Prabowo ini untuk meresmikan dan menyerahkan proyek bantuan air bersih di lima titik. #IndonesiaSentris #IndonesiaHijau #02Melanjutkan #AnakMudaIndonesiaEmas Prabowo Subianto;Sumber Daya Alam']\n",
            "['RT Anies dapat tepuk tangan meriah saat jadi Rektor mewajibkan mata kuliah anti-korupsi untuk memutus mata rantai korupsi.. #AminMiskinkanKoruptor https://t.co/hgXsoQGaEa [RE ekowboy2];Politik']\n",
            "['@CIqXqwGAT04tMtx4OCATxjoVq7vv/Y8HeYaIOgMFg8Y= emng bener sih', ' pendukung 01 ada yg goblok', ' begitu jg dg pendukung 02.. hnya sj menurut pak Ridwan Kamil skemanya terbalik', ' klo 01 MAYORITAS PENDIDIKAN MENENGAH ATAS (artinya ada jg pendidikan rendah yg milih;Demografi']\n",
            "['RT @L3R8XFBw3WGbxRPSj0/0hHZTbqVGX7qtfwRg9zmhK7Q= Sewaktu anies bersikap kritis ke kinerja pak prabowo dianggap engga sopan karena dianggap kurang menghormati orang tua', ' giliran skrg gibran yg tengil dan sok kritis malah dianggap kritis dan keras. Apakah ini tidak standar ganda? Ã°ÂŸÂ¤Â\\xad [RE coldthem];Politik']\n",
            "\n",
            "File read with an alternative delimiter.\n",
            "\n",
            "\n",
            "First line (header) looks like this:\n",
            "Index(['text', 'label'], dtype='object')\n",
            "\n",
            "Each data point looks like this:\n",
            "                                                text             label\n",
            "0  Kunjungan Prabowo ini untuk meresmikan dan men...  Sumber Daya Alam\n",
            "1  RT Anies dapat tepuk tangan meriah saat jadi R...           Politik\n",
            "2  @CIqXqwGAT04tMtx4OCATxjoVq7vv/Y8HeYaIOgMFg8Y= ...         Demografi\n",
            "3  RT @L3R8XFBw3WGbxRPSj0/0hHZTbqVGX7qtfwRg9zmhK7...           Politik\n",
            "4  Anies Baswedan Harap ASN termasuk TNI dan Polr...           Politik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# parse_data_from_file\n",
        "def parse_data_from_file(filename, delimiter=';'):\n",
        "    \"\"\"\n",
        "    Extracts sentences and labels from a CSV file with a specified delimiter\n",
        "\n",
        "    Args:\n",
        "        filename (string): path to the CSV file\n",
        "        delimiter (string): delimiter used in the CSV file\n",
        "\n",
        "    Returns:\n",
        "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=delimiter)\n",
        "        next(reader, None)  # Skip the header row\n",
        "        for row in reader:\n",
        "            sentence = row[0]  # First column\n",
        "            label = row[1]     # Second column\n",
        "            sentences.append(sentence)\n",
        "            labels.append(label)\n",
        "    return sentences, labels\n"
      ],
      "metadata": {
        "id": "wBjtIPdt8Bv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Test your function\n",
        "\n",
        "# With original dataset\n",
        "sentences, labels = parse_data_from_file(\"dataset_penyisihan_bdc_2024.csv\")\n",
        "\n",
        "print(\"ORIGINAL DATASET:\\n\")\n",
        "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
        "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
        "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
        "print(f\"The first 10 labels are {labels[:10]}\\n\\n\")\n",
        "\n",
        "# With a miniature version of the dataset that contains only first 5 rows\n",
        "mini_sentences, mini_labels = parse_data_from_file(\"dataset_penyisihan_bdc_2024.csv\")\n",
        "\n",
        "print(\"MINIATURE DATASET:\\n\")\n",
        "print(f\"There are {len(mini_sentences)} sentences in the miniature dataset.\\n\")\n",
        "print(f\"First sentence has {len(mini_sentences[0].split())} words (after removing stopwords).\\n\")\n",
        "print(f\"There are {len(mini_labels)} labels in the miniature dataset.\\n\")\n",
        "print(f\"The first 10 labels are {mini_labels[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW02KJ2W8L4P",
        "outputId": "50ba2bde-9aff-4344-ca1b-9841282ea1a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL DATASET:\n",
            "\n",
            "There are 5000 sentences in the dataset.\n",
            "\n",
            "First sentence has 20 words (after removing stopwords).\n",
            "\n",
            "There are 5000 labels in the dataset.\n",
            "\n",
            "The first 10 labels are ['Sumber Daya Alam', 'Politik', 'Demografi', 'Politik', 'Politik', 'Politik', 'Pertahanan dan Keamanan', 'Politik', 'Politik', 'Politik']\n",
            "\n",
            "\n",
            "MINIATURE DATASET:\n",
            "\n",
            "There are 5000 sentences in the miniature dataset.\n",
            "\n",
            "First sentence has 20 words (after removing stopwords).\n",
            "\n",
            "There are 5000 labels in the miniature dataset.\n",
            "\n",
            "The first 10 labels are ['Sumber Daya Alam', 'Politik', 'Demografi', 'Politik', 'Politik', 'Politik', 'Pertahanan dan Keamanan', 'Politik', 'Politik', 'Politik']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK and download stopwords\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the text cleaning function\n",
        "def clean_text(text, language='indonesian'):\n",
        "    \"\"\"\n",
        "    Clean text by removing stopwords, special characters, punctuation, mentions, hashtags,\n",
        "    and non-standard alphanumeric strings\n",
        "\n",
        "    Args:\n",
        "        text (string): input text to be cleaned\n",
        "        language (string): language of the stopwords\n",
        "\n",
        "    Returns:\n",
        "        string: cleaned text\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words(language))\n",
        "\n",
        "    # Remove mentions, hashtags, and URLs\n",
        "    text = re.sub(r'(@[A-Za-z0-9_]+)|(#\\w+)|(\\bhttps?://\\S+)', '', text)\n",
        "    # Remove non-standard characters and punctuation, while preserving spaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words and len(word) > 2]\n",
        "\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "\n",
        "# Display the first few rows to understand the structure of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Apply the clean_text function to the text column\n",
        "# Assuming the column containing text data is named 'text'. Adjust as necessary.\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Display the first few rows of the cleaned dataset\n",
        "print(df[['text', 'cleaned_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2X8vwrdHmS8",
        "outputId": "c2dfeb6e-21b8-454c-b693-4dc653381fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text             label\n",
            "0  Kunjungan Prabowo ini untuk meresmikan dan men...  Sumber Daya Alam\n",
            "1  RT Anies dapat tepuk tangan meriah saat jadi R...           Politik\n",
            "2  @CIqXqwGAT04tMtx4OCATxjoVq7vv/Y8HeYaIOgMFg8Y= ...         Demografi\n",
            "3  RT @L3R8XFBw3WGbxRPSj0/0hHZTbqVGX7qtfwRg9zmhK7...           Politik\n",
            "4  Anies Baswedan Harap ASN termasuk TNI dan Polr...           Politik\n",
            "                                                text  \\\n",
            "0  Kunjungan Prabowo ini untuk meresmikan dan men...   \n",
            "1  RT Anies dapat tepuk tangan meriah saat jadi R...   \n",
            "2  @CIqXqwGAT04tMtx4OCATxjoVq7vv/Y8HeYaIOgMFg8Y= ...   \n",
            "3  RT @L3R8XFBw3WGbxRPSj0/0hHZTbqVGX7qtfwRg9zmhK7...   \n",
            "4  Anies Baswedan Harap ASN termasuk TNI dan Polr...   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  Kunjungan Prabowo meresmikan menyerahkan proye...  \n",
            "1  Anies tepuk tangan meriah Rektor mewajibkan ma...  \n",
            "2  YHeYaIOgMFgY emng bener sih pendukung goblok p...  \n",
            "3  hHZTbqVGXqtfwRgzmhKQ anies bersikap kritis kin...  \n",
            "4  Anies Baswedan Harap ASN TNI Polri Pegang Sump...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gunakan Tokenizer dari Keras untuk mengubah teks menjadi fitur numerik."
      ],
      "metadata": {
        "id": "MAuJKjbRJGf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Parameters\n",
        "max_features = 5000\n",
        "max_len = 100\n",
        "\n",
        "# Step 2: Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(df['cleaned_text'])\n",
        "X = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
        "X = pad_sequences(X, maxlen=max_len)\n",
        "\n",
        "# Assuming the label column is named 'label'. Adjust as necessary.\n",
        "y = df['label']\n"
      ],
      "metadata": {
        "id": "5DLzvrg-JBbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Melatih Model dengan TensorFlow\n",
        "Pilih model pembelajaran mesin yang sesuai, seperti LSTM atau model sederhana dengan Dense layers."
      ],
      "metadata": {
        "id": "TIXEnaCFJNJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode the labels for multi-class (assuming y contains categorical labels)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Determine the number of classes in your dataset\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Step 4: Define the model for multi-class classification\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Use softmax for multi-class\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ObKmCrJSql",
        "outputId": "9ce07724-d486-43d5-a4cf-b484840ccbe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "63/63 - 26s - loss: 1.4302 - accuracy: 0.5870 - val_loss: 1.3456 - val_accuracy: 0.5860 - 26s/epoch - 416ms/step\n",
            "Epoch 2/5\n",
            "63/63 - 24s - loss: 1.1478 - accuracy: 0.6400 - val_loss: 1.0203 - val_accuracy: 0.7170 - 24s/epoch - 382ms/step\n",
            "Epoch 3/5\n",
            "63/63 - 22s - loss: 0.7482 - accuracy: 0.7853 - val_loss: 0.9074 - val_accuracy: 0.7250 - 22s/epoch - 348ms/step\n",
            "Epoch 4/5\n",
            "63/63 - 21s - loss: 0.5077 - accuracy: 0.8508 - val_loss: 0.9148 - val_accuracy: 0.7550 - 21s/epoch - 339ms/step\n",
            "Epoch 5/5\n",
            "63/63 - 21s - loss: 0.3615 - accuracy: 0.8932 - val_loss: 0.9698 - val_accuracy: 0.7520 - 21s/epoch - 332ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# download the unlabeled dataset from GitHub"
      ],
      "metadata": {
        "id": "Xu96bqqaJ8jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the unlabeled dataset\n",
        "!wget -O dataset_unlabeled_penyisihan_bdc_2024.csv https://raw.githubusercontent.com/okasugiarta/SATRIA-DATA/main/data/dataset_unlabeled_penyisihan_bdc_2024.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSWLrNRuKA7J",
        "outputId": "61a66925-4bd5-4dbd-8caa-f7818d3c8485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-16 04:03:39--  https://raw.githubusercontent.com/okasugiarta/SATRIA-DATA/main/data/dataset_unlabeled_penyisihan_bdc_2024.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 309666 (302K) [text/plain]\n",
            "Saving to: ‘dataset_unlabeled_penyisihan_bdc_2024.csv’\n",
            "\n",
            "\r          dataset_u   0%[                    ]       0  --.-KB/s               \rdataset_unlabeled_p 100%[===================>] 302.41K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-06-16 04:03:39 (6.19 MB/s) - ‘dataset_unlabeled_penyisihan_bdc_2024.csv’ saved [309666/309666]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediksi pada Unlabeled Dataset\n",
        "Gunakan model yang telah dilatih untuk memprediksi label pada unlabeled dataset."
      ],
      "metadata": {
        "id": "gWtEuGWQN_7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Fungsi untuk membersihkan teks\n",
        "def clean_text(text, language='indonesian'):\n",
        "    stop_words = set(stopwords.words(language))\n",
        "\n",
        "    # Hapus mentions, hashtags, dan URL\n",
        "    text = re.sub(r'(@[A-Za-z0-9_]+)|(#\\w+)|(\\bhttps?://\\S+)', '', text)\n",
        "    # Hapus karakter non-standar dan tanda baca, sambil mempertahankan spasi\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Hapus spasi ekstra\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words and len(word) > 2]\n",
        "\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Parameters\n",
        "max_features = 5000\n",
        "max_len = 100\n",
        "\n",
        "# Load data unlabeled dengan kolom IDText dan Text\n",
        "unlabeled_df = pd.read_csv('dataset_unlabeled_penyisihan_bdc_2024.csv', delimiter=';')\n",
        "\n",
        "# Membersihkan teks pada kolom Text\n",
        "unlabeled_df['cleaned_text'] = unlabeled_df['Text'].apply(clean_text)\n",
        "\n",
        "# Tokenisasi dan padding sekuen teks\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(unlabeled_df['cleaned_text'])\n",
        "X_unlabeled = tokenizer.texts_to_sequences(unlabeled_df['cleaned_text'])\n",
        "X_unlabeled = pad_sequences(X_unlabeled, maxlen=max_len)\n",
        "\n",
        "# Prediksi label untuk data unlabeled dengan model yang sudah dilatih\n",
        "predicted_labels = model.predict(X_unlabeled)\n",
        "\n",
        "# Memilih indeks dengan nilai tertinggi sebagai label prediksi\n",
        "predicted_indices = np.argmax(predicted_labels, axis=1)\n",
        "predicted_classes = label_encoder.inverse_transform(predicted_indices)  # Assuming you have label_encoder\n",
        "\n",
        "# Memasukkan hasil prediksi ke dalam DataFrame\n",
        "unlabeled_df['predicted_label'] = predicted_classes\n",
        "\n",
        "# Menyimpan hasil prediksi ke file CSV atau menampilkan di layar\n",
        "print(unlabeled_df[['IDText', 'Text', 'predicted_label']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DbO_VbDNspa",
        "outputId": "f6acd5b7-becf-4147-9e64-3841b455981d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 1s 34ms/step\n",
            "      IDText                                               Text  \\\n",
            "0    TXT0001  Lu mau org2 pro-demokrasi di negara ini bisa p...   \n",
            "1    TXT0002  Prabowo ditanya soal hutang luar negeri dia me...   \n",
            "2    TXT0003  kiki_daliyo  Ganjar Pranowo itulah beliau soso...   \n",
            "3    TXT0004  @kumparan Prabowo Gibran yang bisa melakukan i...   \n",
            "4    TXT0005  @sniperruben45 @uda_zulhendra @ainunnajib Lah ...   \n",
            "..       ...                                                ...   \n",
            "995  TXT0996  Bikin bangga deh, Ganjar-Mahfud mau alokasikan...   \n",
            "996  TXT0997  Pak Jokowi sebelum pilpres 2024 berbesar hati ...   \n",
            "997  TXT0998  @datuakrajoangek Sbaiknya si gemot nga usah ik...   \n",
            "998  TXT0999  kebiasaan merembuk atau bermusyawarah jadi gay...   \n",
            "999  TXT1000  Mirage Ditolak Juwono, Dibeli Prabowo, Jubir T...   \n",
            "\n",
            "     predicted_label  \n",
            "0                  4  \n",
            "1                  5  \n",
            "2                  5  \n",
            "3                  5  \n",
            "4                  5  \n",
            "..               ...  \n",
            "995                5  \n",
            "996                5  \n",
            "997                5  \n",
            "998                5  \n",
            "999                4  \n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the labeled dataset\n",
        "df_train = pd.read_csv('dataset_penyisihan_bdc_2024.csv', delimiter=';')\n",
        "\n",
        "# Encode labels in the training dataset\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(df_train['label'])\n",
        "\n",
        "# Assuming you have already predicted labels in `unlabeled_df['predicted_label']`\n",
        "# In your case, it's `unlabeled_df['predicted_label']`\n",
        "\n",
        "# Decode predicted labels back to original text labels\n",
        "predicted_labels_text = label_encoder.inverse_transform(unlabeled_df['predicted_label'])\n",
        "\n",
        "# Insert the predicted labels into the DataFrame\n",
        "unlabeled_df['predicted_label_text'] = predicted_labels_text\n",
        "\n",
        "# Display or save the relevant columns\n",
        "print(unlabeled_df[['IDText', 'Text', 'predicted_label_text']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdfu0UqDSqp4",
        "outputId": "61d567ab-6fe8-4ee2-b6fc-f5f033099eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      IDText                                               Text  \\\n",
            "0    TXT0001  Lu mau org2 pro-demokrasi di negara ini bisa p...   \n",
            "1    TXT0002  Prabowo ditanya soal hutang luar negeri dia me...   \n",
            "2    TXT0003  kiki_daliyo  Ganjar Pranowo itulah beliau soso...   \n",
            "3    TXT0004  @kumparan Prabowo Gibran yang bisa melakukan i...   \n",
            "4    TXT0005  @sniperruben45 @uda_zulhendra @ainunnajib Lah ...   \n",
            "..       ...                                                ...   \n",
            "995  TXT0996  Bikin bangga deh, Ganjar-Mahfud mau alokasikan...   \n",
            "996  TXT0997  Pak Jokowi sebelum pilpres 2024 berbesar hati ...   \n",
            "997  TXT0998  @datuakrajoangek Sbaiknya si gemot nga usah ik...   \n",
            "998  TXT0999  kebiasaan merembuk atau bermusyawarah jadi gay...   \n",
            "999  TXT1000  Mirage Ditolak Juwono, Dibeli Prabowo, Jubir T...   \n",
            "\n",
            "        predicted_label_text  \n",
            "0    Pertahanan dan Keamanan  \n",
            "1                    Politik  \n",
            "2                    Politik  \n",
            "3                    Politik  \n",
            "4                    Politik  \n",
            "..                       ...  \n",
            "995                  Politik  \n",
            "996                  Politik  \n",
            "997                  Politik  \n",
            "998                  Politik  \n",
            "999  Pertahanan dan Keamanan  \n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE TO EXCEL"
      ],
      "metadata": {
        "id": "QxGBisalXCw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path and file name for saving the Excel file\n",
        "output_file = 'SD2024040000423.csv'\n",
        "\n",
        "# Save the DataFrame to Excel with comma delimiter\n",
        "unlabeled_df.to_csv(output_file, sep=',', index=False)"
      ],
      "metadata": {
        "id": "ZBWXUh2oTIHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}